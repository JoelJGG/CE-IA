"""
Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17utMjGRh-_tx9FJowGPM18vIo-R4xuzP

# importing json file
"""

import json
import numpy as np # The data structure I use
import matplotlib.pyplot as plt # To show my graphs
import scipy.io as sio # Audio imput (wavfile.read)
from IPython.display import Audio # In order to reproduce my audio
from numpy.fft import fft, 
from google.colab import files

# Upload the file
data = files.upload()

audio_uploaded = files.upload()

import scipy.io as sio #(wavfile.read)

# Access the content of my json
file_content = data['vocales.json'].decode('utf-8')
parsed_data = json.loads(file_content)

start = float(parsed_data[30]["start"])
end = float(parsed_data[30]["end"])

select = 19 #19 = E
start = float(parsed_data[select]["start"])
end=float(parsed_data[select]["end"])

Fs, audio = sio.wavfile.read("AudioIA.wav")
cut = audio[int(start*Fs):int(end*Fs),0]

Audio(cut, rate=Fs)

#Printing the audio after being cut
plt.figure()
plt.plot(cut)
plt.title("")
plt.xlabel("time")
plt.ylabel("Amplitud")
plt.show()

# As it is hard to understand I apply fourier
fourier = fft(cut)
plt.figure()
plt.plot(fourier)
plt.title("FFT")
plt.xlabel("frequency")
plt.ylabel("Amplitude")
plt.show()

#Getting only the close to 0 values
Fsmall = fourier[0:300]
plt.figure()
plt.plot(Fsmall)
plt.title("FFT")
plt.xlabel("frequency")
plt.ylabel("Amplitude")
plt.show()

# I don't care about the fase, just the module
toprocess = np.sqrt((np.real(Fsmall)**2+np.imag(Fsmall)**2))
plt.figure()
plt.plot(toprocess)
plt.show()

# Applying a filter to get rid of the noise


#This filter is about the vocal cords oscillation
#The higher the number, flatter result
filter = 7
out = np.zeros(len(toprocess)-filter, dtype=np.float64)
for i in range(len(toprocess)-filter):
    for j in range(filter):
        out[i] += toprocess[i+j]


out[0:15] = 0

plt.figure()
plt.plot(out)
plt.show()
np.argmax(out)

#This filter decides how much I cut in order to isolate the maximumx of the 3 main frequencies
filter2 = 50

maxim1 = np.argmax(out)

filmin = filter2
if maxim1 < filter2:
    filmin = maxim1

out[maxim1-filmin:maxim1+filter2] = 0
plt.figure()
plt.plot(out)
plt.show()

maxim2 = np.argmax(out)

out[maxim2-filter2:maxim2+filter2] = 0
plt.figure()
plt.plot(out)
plt.show()


maxim3 = np.argmax(out)

out[maxim3-filter2:maxim3+filter2] = 0
plt.figure()
plt.plot(out)
plt.show()

from sklearn.metrics import ConfusionMatrixDisplay, accuracy_score
from sklearn.svm import SVC
import matplotlib.pyplot as plt
import random

def wav2vec(filepath, start, end):
    # Reading audio file
    Fs, audio = sio.wavfile.read(filepath)
    # Cutting the audio in the defined intervals
    cut = audio[int(start * Fs):int(end * Fs), 0]
    # Realizing the fft
    fourier = fft(cut)
    Fsmall = fourier[:300]
    # Calculate the magnitude spectrum
    toprocess = np.sqrt(np.real(Fsmall) ** 2 + np.imag(Fsmall) ** 2)

    # Applying a mobile media filter
    filter_size = 7
    out = np.convolve(toprocess, np.ones(filter_size) / filter_size, mode='valid')
    out[:15] = 0  # Deleting the low frequencies

    # Extracting the three main frequency peaks
    frequencies = []
    filter2 = 55
    for _ in range(3):  
        max_idx = np.argmax(out)
        frequency = max_idx * Fs / len(fourier)
        frequencies.append(frequency)  # Solo agregar el valor de frecuencia
        out[max(0, max_idx - filter2):min(len(out), max_idx + filter2)] = 0

    # Returning the main frequencies as a numpy array
    return np.array(frequencies)  

# Setting my letters to float and back
def gestionaY(y):
    label_dict = {"A": 1.0, "E": 2.0, "I": 3.0, "O": 4.0, "U": 5.0}
    return label_dict.get(y, 0)

def gestionaYInversa(y):
    inverse_label_dict = {1.0: "A", 2.0: "E", 3.0: "I", 4.0: "O", 5.0: "U"}
        return inverse_label_dict.get(y)

X = np.zeros((len(parsed_data),3))
y = np.zeros(len(parsed_data))
for select in range(len(parsed_data)):
    start = float(parsed_data[select]["start"])
    end = float(parsed_data[select]["end"])
    vocal = parsed_data[select]["vocal"]

    # Extract audio caracteristics using wav2vc
    features = wav2vec("AudioIA.wav", start, end)

    X[select] = features
    y[select] = gestionaY(vocal)

X.reshape(len(parsed_data), -1)

# Initializing the classifier
model = SVC(random_state=0)

def split(X, y, percentage):
    number_of_elements = int(len(X) * percentage)

    # picking random indexes for the test 
    test_indexes = np.random.choice(len(X), size=number_of_elements, replace=False)

    # Obtainign the remaining indexes for training
    training_indexes = np.setdiff1d(np.arange(len(X)), test_indexes)

    # Creating the test and training groups
    X_test, y_test = X[test_indexes], y[test_indexes]
    X_training, y_training = X[training_indexes], y[training_indexes]

    return X_test, X_training, y_test, y_training
X_test,X_train,y_test,y_train = split(X,y,0.1)

model.fit(X_train, y_train)
y_pred = model.predict(X_test)
gestionaYInversa_vect = np.vectorize(gestionaYInversa)

# Changing the labels back to Letters in order to see the Matrix more clearly
y_test_labels = gestionaYInversa_vect(y_test)
y_pred_labels = gestionaYInversa_vect(y_pred)


ConfusionMatrixDisplay.from_predictions(y_test_labels, y_pred_labels)
plt.show()

accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.4f}")
